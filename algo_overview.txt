* Question Answering models, given a paragraph of text and a question, the model looks for the answer in the paragraph.
* A subfield of Question Answering called Reading Comprehension.
* Several models have already outperformed human on SQuAD dataset. But making this data took a huge human effort of labelling over 100 000 context-question-answer triplets.
* What it you want to take it to another language, or don't have this labeled data?
* This is why un/semi-supervised approaches have come, like: Unsupervised Question Answering by Cloze Translation.
* This approach can be broken down as follow:
  - Obtaining the context
  - Defining the answers
  - Generating cloze statements
  - Translating cloze to natural questions
  - Training a QA model
* The major challenge is generating a right question containing enough information for the QA model to know where to look for the answer.
* For this, first generate cloze statements using the context and the answer. Then translate the cloze statements to natural questions.
* Cloze generation:
  - For cloze generation they used Wikipedia’s database dumps which is an XML and to convert it to text they used wikiextractor.
  - To extract context, simply divide the article into fixed length paragraphs (be careful, these contexts will later be fed into the QA models, so the context length is constrained by computer memory).
* Defining the answers:
  - Choose answer from the given context, BEFORE generating the question.
  - A simple way is to use named entities (person, place etc.), they used spaCy.
  - Now store named entity as the answer and the paragraph's starting and ending positions as the context, and also the named entity's label which will be used to generate question.
* Obtaining cloze statements
  - It is a phrase with a blanked out word, like: ""I love ____.""
  - Here cloze statement is the statement containing the chosen answer and this answer is replaced by a mask.
  - Not all information in the sentence is not necessarily relevant to the question. They use a constituency parser from allennlp to build a tree breaking the sentence into its structural constituents then extract the sub-phrase that contains the answer.
* Translating into natural questions
  - Identy mapping: a heuristic approach...simply replacing the mask by an appropriate question word and appending a question mark.
  - Noisy Clozes: also a heuristic approach...it assumes the difference between the cloze statements and natural questions are perturbations. Natural questions is the perturbated form of the cloze statements.
        - dropping words in cloze statement with a probability p (=0.1).
        - shuffling words in the statement, but to prevent the output to be completely in random order they added a constant k: for each i-th word in our input sentence, its position in the output σ(i) must verify |σ(i) − i| ≤ k. That is, each shuffled word cannot be too far from its original position. They used k = 3.        - After adding noise, they simply remove the mask (older one), prepend the associated question word, and append a question mark.
  - Unsupervised Neural Machine Translation (UNMT): 
        - Need two large corpora of data for each language
        - The advantage of unsupervised NMT is that the two corpora need not be parallel.
        - Train two language models in each language, Pₛ and Pₜ each having an encoder and a decoder.
        - The Language Model (LM) receives input text (noise added text). Noise generated by dropping and shuffling as discussed. Moreover they mask certain words with a probability p = 0.1.
        - The output is compated to the original text

REFERENCE:
https://medium.com/illuin/unsupervised-question-answering-4758e5f2be9b
